mplot_response(tag = testSet$Revenue,
score =predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])
mplot_gain(tag = testSet$Revenue,
score =predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])
mplot_conf(tag = testSet$Revenue,
score =predict(fit.xgb, testSet,type = "prob")[["second_class"]],model_name = "XGBoost")
#-----------Compare Confusion Matrix------------------------
mplot_conf(tag = testSet$Revenue,
score =predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]],model_name = "XGBoost Smote")
eval_models_xgb
confusion_matrix_xgb1
confusion_matrix_xgb0
confusion_matrix_xgb1
confusion_matrix_xgb0
confusion_matrix_xgb1$byClass
confusion_matrix_xgb0$table
confusion_matrix_xgb1$table
#---------------------------------Machine Learning!--------------------
# H2O.ai
h2o.init(
ip = "localhost",
nthreads = -1,
max_mem_size = "4g"
)
h2o.confusionMatrix(model2, valid=T)
model1 <- h2o.loadModel(path = "ML_models/StackedEnsemble_BestOfFamily_AutoML_20201021_084846")
model2 <- h2o.loadModel(path = "ML_models/GBM_grid__1_AutoML_20201021_084846_model_60")
model3 <- h2o.loadModel(path = "ML_models/GBM_grid__1_AutoML_20201021_084846_model_32")
model_dp <- h2o.loadModel(path = "ML_models/model_dp")
model_glm <- h2o.loadModel(path = "ML_models/model_glm")
h2o.confusionMatrix(model1, valid=T)
confusion_matrix_xgb1$table
179 / (179+331)
confusion_matrix_xgb0$table
77 / (77+226)
eval_models_xgb
confusion_matrix_xgb1
confusion_matrix_xgb0
confusion_matrix_xgb1$positive
confusion_matrix_xgb1$
confusion_matrix_xgb1$byClass
confusion_matrix_xgb0$byClass
confusion_matrix_xgb0$byClass[3]
confusion_matrix_xgb0$table
confusion_matrix_xgb1$table
confusion_matrix_xgb1$byClass[3]
confusion_matrix_xgb0$byClass[3]
confusion_matrix_xgb1$byClass[3]
predict(smote_fit.xgb, testSet,type = "prob")
confusion_matrix_xgb1$table
331 / (331+50)
50 / (331+50)
confusion_matrix_xgb1$table
confusion_matrix_xgb1 <- cbind(testSet$Revenue,eval) %>%
data.frame() %>%
table() %>% confusionMatrix()
confusion_matrix_xgb1$table
confusion_matrix_xgb1 <- cbind(testSet$Revenue,eval) %>%
data.frame() %>%
table() %>% confusionMatrix()
eval_models_xgb # XGBoost Smote Wins!!
models <- c("XGB", "XGB Smote")
error_class_1 <- round(c(confusion_matrix_xgb0$table[2,1] /
(confusion_matrix_xgb0$table[2,1] + confusion_matrix_xgb0$table[2,2]),
confusion_matrix_xgb1$table[2,1] /
(confusion_matrix_xgb1$table[2,1] + confusion_matrix_xgb1$table[2,2])),4)
logloss_ <- round(c(logLoss(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
logLoss(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
auc_ <- round(c(auc(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
auc(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
eval_models_xgb <- data.frame(models, auc_, logloss_, error_class_1, stringsAsFactors = F) %>%
arrange(desc(auc_))
eval_models_xgb
eval_models_xgb # XGBoost Smote Wins!!
mplot_conf(tag = testSet$Revenue,
score =predict(fit.xgb, testSet,type = "prob")[["second_class"]],model_name = "XGBoost")
#-----------Compare Confusion Matrix------------------------
mplot_conf(tag = testSet$Revenue,
score =predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]],model_name = "XGBoost Smote")
source('C:/Users/azaze/Desktop/dataset/online_shoppers_intention/online_shoppers_intention/load_clean_transform.R', echo=TRUE)
source('C:/Users/azaze/Desktop/dataset/online_shoppers_intention/online_shoppers_intention/load_clean_transform.R', echo=TRUE)
#------------------------------------Looking for best tuning parameters -----------------------------------------
param <-  expand.grid(nrounds = c(50,100),
max_depth = c(5, 6, 10, 12),
eta = c(0.3, 0.1, 0.03), gamma = c(0.5, 1),
colsample_bytree = c(0.8, 0.9, 1), min_child_weight = 1,
subsample = c(0.5,0.8, 1))
nrows(param)
nrow(param)
source('C:/Users/azaze/Desktop/dataset/online_shoppers_intention/online_shoppers_intention/load_clean_transform.R', echo=TRUE)
#---------------------------------Machine Learning!--------------------
# H2O.ai
h2o.init(
ip = "localhost",
nthreads = -1,
max_mem_size = "4g"
)
df1<- df
df1$Month <- as.character(df1$Month) # H2O don't allows Ordered factors
df_h2o<- as.h2o(df1)
df_h2o$Month <- h2o.asfactor(df_h2o$Month)
h2o.describe(df_h2o)
#----------------------------------
#--------------------------------Train/Test split-----------------------------------------------
particiones <- h2o.splitFrame(data = df_h2o, ratios = c(0.8), seed = 33)
train <- h2o.assign(data = particiones[[1]], key = "train")
test <- h2o.assign(data = particiones[[2]], key = "test")
h2o.table(train$Revenue)/h2o.nrow(train$Revenue)
h2o.table(test$Revenue)/h2o.nrow(test$Revenue)
# Really unbalanced classes, this problem will be solved by
# adjust some arguments in the model construction
train$Revenue <- h2o.asfactor(train$Revenue)
test$Revenue <- h2o.asfactor(test$Revenue)
y <- "Revenue"
x <-setdiff(h2o.colnames(df_h2o),y)
df$Month <- as.numeric(df$Month)
df1<-df
#-------Transform all categorical variables to factor, to allows us convert it to dummies variables
for (col in colnames(df1)){
if (n_distinct(df1[[col]]) > 2 & n_distinct(df1[[col]]) < 21){
df1[[col]] <- factor(df1[[col]])
}
}
df1<- dummy.data.frame(df1, dummy.classes = "factor")
for (col in colnames(df1)){
if (class(df1[[col]]) == "factor"){
df1[[col]] <- as.numeric(df1[[col]])
}
}
#------------------Train/Test split----------------------------
validation_index<-createDataPartition(df1$Revenue, p=0.8, list=FALSE)
testSet<-df1[-validation_index,]
trainSet<-df1[validation_index,]
model1 <- h2o.loadModel(path = "ML_models/StackedEnsemble_BestOfFamily_AutoML_20201021_084846")
model2 <- h2o.loadModel(path = "ML_models/GBM_grid__1_AutoML_20201021_084846_model_60")
model3 <- h2o.loadModel(path = "ML_models/GBM_grid__1_AutoML_20201021_084846_model_32")
model_dp <- h2o.loadModel(path = "ML_models/model_dp")
model_glm <- h2o.loadModel(path = "ML_models/model_glm")
load(paste(getwd(),"/ML_models/fit.xgb.RData", sep = ""))
trainSet$Revenue<- as.factor(trainSet$Revenue)
smoted_data <- SMOTE(Revenue ~ ., trainSet, perc.over=100)
smoted_data$Revenue<- as.numeric(smoted_data$Revenue)
prop.table(table(smoted_data$Revenue)) # equal proportions
load(paste(getwd(),"/ML_models/smote_fit.xgb.RData", sep = ""))
eval <- predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb0 <- cbind(eval, testSet$Revenue) %>%
data.frame() %>%
table() %>% confusionMatrix()
eval <- predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb1 <- cbind(testSet$Revenue,eval) %>%
data.frame() %>%
table() %>% confusionMatrix()
models <- c("XGB", "XGB Smote")
error_class_1 <- round(c(confusion_matrix_xgb0$table[2,1] /
(confusion_matrix_xgb0$table[2,1] + confusion_matrix_xgb0$table[2,2]),
confusion_matrix_xgb1$table[2,1] /
(confusion_matrix_xgb1$table[2,1] + confusion_matrix_xgb1$table[2,2])),4)
logloss_ <- round(c(logLoss(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
logLoss(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
auc_ <- round(c(auc(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
auc(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
eval_models_xgb <- data.frame(models, auc_, logloss_, error_class_1, stringsAsFactors = F) %>%
arrange(desc(auc_))
eval_models_xgb # XGBoost Smote
#-----------Compare Confusion Matrix------------------------
mplot_conf(tag = testSet$Revenue,
score =predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]],model_name = "XGBoost Smote")
mplot_conf(tag = testSet$Revenue,
score =predict(fit.xgb, testSet,type = "prob")[["second_class"]],model_name = "XGBoost")
source('C:/Users/azaze/Desktop/dataset/online_shoppers_intention/online_shoppers_intention/load_clean_transform.R', echo=TRUE)
library(knitr)
unlink('Reporte_técnico_cache', recursive = TRUE)
unlink('Reporte_técnico_cache', recursive = TRUE)
eval_models_xgb # XGBoost Smote Wins!!
knit_with_parameters('C:/Users/azaze/Desktop/dataset/online_shoppers_intention/online_shoppers_intention/Reporte_técnico.Rmd', encoding = 'UTF-8')
str(knitr::opts_chunk$get())
library(devtools)
source('C:/Users/azaze/Desktop/dataset/online_shoppers_intention/online_shoppers_intention/load_clean_transform.R', echo=TRUE)
h2o.init(
ip = "localhost",
nthreads = -1,
max_mem_size = "4g"
)
df1<- df
df1$Month <- as.character(df1$Month) # H2O don't allows Ordered factors
df_h2o<- as.h2o(df1)
df_h2o$Month <- h2o.asfactor(df_h2o$Month)
h2o.describe(df_h2o)
#----------------------------------
#--------------------------------Train/Test split-----------------------------------------------
particiones <- h2o.splitFrame(data = df_h2o, ratios = c(0.8), seed = 33)
train <- h2o.assign(data = particiones[[1]], key = "train")
test <- h2o.assign(data = particiones[[2]], key = "test")
h2o.table(train$Revenue)/h2o.nrow(train$Revenue)
h2o.table(test$Revenue)/h2o.nrow(test$Revenue)
# Really unbalanced classes, this problem will be solved by
# adjust some arguments in the model construction
train$Revenue <- h2o.asfactor(train$Revenue)
test$Revenue <- h2o.asfactor(test$Revenue)
y <- "Revenue"
x <-setdiff(h2o.colnames(df_h2o),y)
df$Month <- as.numeric(df$Month)
df1<-df
#-------Transform all categorical variables to factor, to allows us convert it to dummies variables
for (col in colnames(df1)){
if (n_distinct(df1[[col]]) > 2 & n_distinct(df1[[col]]) < 21){
df1[[col]] <- factor(df1[[col]])
}
}
df1<- dummy.data.frame(df1, dummy.classes = "factor")
for (col in colnames(df1)){
if (class(df1[[col]]) == "factor"){
df1[[col]] <- as.numeric(df1[[col]])
}
}
#------------------Train/Test split----------------------------
validation_index<-createDataPartition(df1$Revenue, p=0.8, list=FALSE)
testSet<-df1[-validation_index,]
trainSet<-df1[validation_index,]
model1 <- h2o.loadModel(path = "ML_models/StackedEnsemble_BestOfFamily_AutoML_20201021_084846")
model2 <- h2o.loadModel(path = "ML_models/GBM_grid__1_AutoML_20201021_084846_model_60")
model3 <- h2o.loadModel(path = "ML_models/GBM_grid__1_AutoML_20201021_084846_model_32")
model_dp <- h2o.loadModel(path = "ML_models/model_dp")
model_glm <- h2o.loadModel(path = "ML_models/model_glm")
load(paste(getwd(),"/ML_models/fit.xgb.RData", sep = ""))
eval <- predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb0 <- cbind(eval, testSet$Revenue) %>%
data.frame() %>%
table() %>% confusionMatrix()
trainSet$Revenue<- as.factor(trainSet$Revenue)
smoted_data <- SMOTE(Revenue ~ ., trainSet, perc.over=100)
smoted_data$Revenue<- as.numeric(smoted_data$Revenue)
prop.table(table(smoted_data$Revenue)) # equal proportions
load(paste(getwd(),"/ML_models/smote_fit.xgb.RData", sep = ""))
eval <- predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb1 <- cbind(testSet$Revenue,eval) %>%
data.frame() %>%
table() %>% confusionMatrix()
models <- c("XGB", "XGB Smote")
error_class_1 <- round(c(confusion_matrix_xgb0$table[2,1] /
(confusion_matrix_xgb0$table[2,1] + confusion_matrix_xgb0$table[2,2]),
confusion_matrix_xgb1$table[2,1] /
(confusion_matrix_xgb1$table[2,1] + confusion_matrix_xgb1$table[2,2])),4)
logloss_ <- round(c(logLoss(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
logLoss(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
auc_ <- round(c(auc(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
auc(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
eval_models_xgb <- data.frame(models, auc_, logloss_, error_class_1, stringsAsFactors = F) %>%
arrange(desc(auc_))
eval_models_xgb
set.seed(33)
validation_index<-createDataPartition(df1$Revenue, p=0.8, list=FALSE)
testSet<-df1[-validation_index,]
trainSet<-df1[validation_index,]
eval <- predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb0 <- cbind(eval, testSet$Revenue) %>%
data.frame() %>%
table() %>% confusionMatrix()
trainSet$Revenue<- as.factor(trainSet$Revenue)
smoted_data <- SMOTE(Revenue ~ ., trainSet, perc.over=100)
smoted_data$Revenue<- as.numeric(smoted_data$Revenue)
prop.table(table(smoted_data$Revenue)) # equal proportions
eval <- predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb1 <- cbind(testSet$Revenue,eval) %>%
data.frame() %>%
table() %>% confusionMatrix()
models <- c("XGB", "XGB Smote")
error_class_1 <- round(c(confusion_matrix_xgb0$table[2,1] /
(confusion_matrix_xgb0$table[2,1] + confusion_matrix_xgb0$table[2,2]),
confusion_matrix_xgb1$table[2,1] /
(confusion_matrix_xgb1$table[2,1] + confusion_matrix_xgb1$table[2,2])),4)
logloss_ <- round(c(logLoss(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
logLoss(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
auc_ <- round(c(auc(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
auc(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
eval_models_xgb <- data.frame(models, auc_, logloss_, error_class_1, stringsAsFactors = F) %>%
arrange(desc(auc_))
eval_models_xgb # XGBoost Smote Wins!!
set.seed(42)
validation_index<-createDataPartition(df1$Revenue, p=0.8, list=FALSE)
testSet<-df1[-validation_index,]
trainSet<-df1[validation_index,]
eval <- predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb0 <- cbind(eval, testSet$Revenue) %>%
data.frame() %>%
table() %>% confusionMatrix()
models <- c("Stacked Ensemble auto", "GBM0 auto","GBM1 auto","GLM","Deep Learning","XGB")
error_class_1 <- round(c(h2o.confusionMatrix(model1, valid=T)[,3][2],
h2o.confusionMatrix(model2, valid=T)[,3][2],
h2o.confusionMatrix(model3, valid=T)[,3][2],
h2o.confusionMatrix(model_glm, valid=T)[,3][2],
h2o.confusionMatrix(model_dp, valid=T)[,3][2],
confusion_matrix_xgb0$table[2,1] /
(confusion_matrix_xgb0$table[2,1] + confusion_matrix_xgb0$table[2,2])),4)
logloss_ <- round(c(h2o.logloss(h2o.performance(model1, valid = T)),
h2o.logloss(h2o.performance(model2, valid = T)),
h2o.logloss(h2o.performance(model3, valid = T)),
h2o.logloss(h2o.performance(model_glm, valid = T)),
h2o.logloss(h2o.performance(model_dp, valid = T)),
logLoss(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]])),4)
auc_ <- round(c(h2o.auc(h2o.performance(model1, valid = T)),
h2o.auc(h2o.performance(model2, valid = T)),
h2o.auc(h2o.performance(model3, valid = T)),
h2o.auc(h2o.performance(model_glm, valid = T)),
h2o.auc(h2o.performance(model_dp, valid = T)),
auc(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]])),4)
eval_models <- data.frame(models, auc_, logloss_, error_class_1, stringsAsFactors = F) %>%
arrange(desc(auc_))
eval_models
trainSet$Revenue<- as.factor(trainSet$Revenue)
smoted_data <- SMOTE(Revenue ~ ., trainSet, perc.over=100)
smoted_data$Revenue<- as.numeric(smoted_data$Revenue)
prop.table(table(smoted_data$Revenue)) # equal proportions
eval <- predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb1 <- cbind(testSet$Revenue,eval) %>%
data.frame() %>%
table() %>% confusionMatrix()
models <- c("XGB", "XGB Smote")
error_class_1 <- round(c(confusion_matrix_xgb0$table[2,1] /
(confusion_matrix_xgb0$table[2,1] + confusion_matrix_xgb0$table[2,2]),
confusion_matrix_xgb1$table[2,1] /
(confusion_matrix_xgb1$table[2,1] + confusion_matrix_xgb1$table[2,2])),4)
logloss_ <- round(c(logLoss(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
logLoss(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
auc_ <- round(c(auc(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
auc(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
eval_models_xgb <- data.frame(models, auc_, logloss_, error_class_1, stringsAsFactors = F) %>%
arrange(desc(auc_))
eval_models_xgb # XGBoost Smote Wins!!
#------------------Train/Test split----------------------------
set.seed(1991)
validation_index<-createDataPartition(df1$Revenue, p=0.8, list=FALSE)
testSet<-df1[-validation_index,]
trainSet<-df1[validation_index,]
eval <- predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb0 <- cbind(eval, testSet$Revenue) %>%
data.frame() %>%
table() %>% confusionMatrix()
trainSet$Revenue<- as.factor(trainSet$Revenue)
smoted_data <- SMOTE(Revenue ~ ., trainSet, perc.over=100)
smoted_data$Revenue<- as.numeric(smoted_data$Revenue)
prop.table(table(smoted_data$Revenue)) # equal proportions
eval <- predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb1 <- cbind(testSet$Revenue,eval) %>%
data.frame() %>%
table() %>% confusionMatrix()
models <- c("XGB", "XGB Smote")
error_class_1 <- round(c(confusion_matrix_xgb0$table[2,1] /
(confusion_matrix_xgb0$table[2,1] + confusion_matrix_xgb0$table[2,2]),
confusion_matrix_xgb1$table[2,1] /
(confusion_matrix_xgb1$table[2,1] + confusion_matrix_xgb1$table[2,2])),4)
logloss_ <- round(c(logLoss(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
logLoss(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
auc_ <- round(c(auc(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
auc(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
eval_models_xgb <- data.frame(models, auc_, logloss_, error_class_1, stringsAsFactors = F) %>%
arrange(desc(auc_))
eval_models_xgb # XGBoost Smote Wins!!
#------------------Train/Test split----------------------------
set.seed(33)
validation_index<-createDataPartition(df1$Revenue, p=0.66, list=FALSE)
testSet<-df1[-validation_index,]
trainSet<-df1[validation_index,]
eval <- predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb0 <- cbind(eval, testSet$Revenue) %>%
data.frame() %>%
table() %>% confusionMatrix()
trainSet$Revenue<- as.factor(trainSet$Revenue)
smoted_data <- SMOTE(Revenue ~ ., trainSet, perc.over=100)
smoted_data$Revenue<- as.numeric(smoted_data$Revenue)
prop.table(table(smoted_data$Revenue))
eval <- predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb1 <- cbind(testSet$Revenue,eval) %>%
data.frame() %>%
table() %>% confusionMatrix()
models <- c("XGB", "XGB Smote")
error_class_1 <- round(c(confusion_matrix_xgb0$table[2,1] /
(confusion_matrix_xgb0$table[2,1] + confusion_matrix_xgb0$table[2,2]),
confusion_matrix_xgb1$table[2,1] /
(confusion_matrix_xgb1$table[2,1] + confusion_matrix_xgb1$table[2,2])),4)
logloss_ <- round(c(logLoss(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
logLoss(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
auc_ <- round(c(auc(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
auc(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
eval_models_xgb <- data.frame(models, auc_, logloss_, error_class_1, stringsAsFactors = F) %>%
arrange(desc(auc_))
eval_models_xgb # XGBoost Smote Wins!!
set.seed(33)
validation_index<-createDataPartition(df1$Revenue, p=0.7, list=FALSE)
testSet<-df1[-validation_index,]
trainSet<-df1[validation_index,]
eval <- predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb0 <- cbind(eval, testSet$Revenue) %>%
data.frame() %>%
table() %>% confusionMatrix()
trainSet$Revenue<- as.factor(trainSet$Revenue)
smoted_data <- SMOTE(Revenue ~ ., trainSet, perc.over=100)
smoted_data$Revenue<- as.numeric(smoted_data$Revenue)
load(paste(getwd(),"/ML_models/smote_fit.xgb.RData", sep = ""))
eval <- predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb1 <- cbind(testSet$Revenue,eval) %>%
data.frame() %>%
table() %>% confusionMatrix()
models <- c("XGB", "XGB Smote")
error_class_1 <- round(c(confusion_matrix_xgb0$table[2,1] /
(confusion_matrix_xgb0$table[2,1] + confusion_matrix_xgb0$table[2,2]),
confusion_matrix_xgb1$table[2,1] /
(confusion_matrix_xgb1$table[2,1] + confusion_matrix_xgb1$table[2,2])),4)
logloss_ <- round(c(logLoss(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
logLoss(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
auc_ <- round(c(auc(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
auc(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
eval_models_xgb <- data.frame(models, auc_, logloss_, error_class_1, stringsAsFactors = F) %>%
arrange(desc(auc_))
eval_models_xgb # XGBoost Smote Wins!!
#-----------Compare Confusion Matrix------------------------
mplot_conf(tag = testSet$Revenue,
score =predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]],model_name = "XGBoost Smote")
mplot_conf(tag = testSet$Revenue,
score =predict(fit.xgb, testSet,type = "prob")[["second_class"]],model_name = "XGBoost")
confusion_matrix_xgb1
confusion_matrix_xgb0
confusion_matrix_xgb1 <- cbind(testSet$Revenue,eval) %>%
data.frame() %>%
table() %>% confusionMatrix()
confusion_matrix_xgb1
testSet$Revenue
predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
#----------------
precision(testSet$Revenue,predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 )
#----------------
recall(testSet$Revenue,predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 )
precision(testSet$Revenue,predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 )
recall(testSet$Revenue,predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 )
precision(testSet$Revenue,predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 )
precision_ <- round(c(precision(testSet$Revenue,predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 ),
precision(testSet$Revenue,predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5)),4)
recall_ <- round(c(recall(testSet$Revenue,predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 ),
recall(testSet$Revenue,predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 ),4))
eval_models_xgb <- data.frame(models, auc_, logloss_, error_class_1, precision_, recall_, stringsAsFactors = F) %>%
arrange(desc(auc_))
recall_ <- round(c(recall(testSet$Revenue,predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 ),
recall(testSet$Revenue,predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 ),4))
eval_models_xgb <- data.frame(models, auc_, logloss_, error_class_1, precision_, recall_, stringsAsFactors = F) %>%
arrange(desc(auc_))
precision(testSet$Revenue,predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 )
precision(testSet$Revenue,predict(fit.xgb, testSet,type = "prob")[["second_class"]])
precision(testSet$Revenue,predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 )
precision_
recall_
#----------------
recall(testSet$Revenue,predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 )
recall(testSet$Revenue,predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 )
recall_ <- round(c(recall(testSet$Revenue,predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 ),
recall(testSet$Revenue,predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 )),4)
eval_models_xgb <- data.frame(models, auc_, logloss_, error_class_1, precision_, recall_, stringsAsFactors = F) %>%
arrange(desc(auc_))
eval_models_xgb # XGBoost Smote Wins!!
source('C:/Users/azaze/Desktop/dataset/online_shoppers_intention/online_shoppers_intention/load_clean_transform.R', echo=TRUE)
df$Month <- as.numeric(df$Month)
df1<-df
#-------Transform all categorical variables to factor, to allows us convert it to dummies variables
for (col in colnames(df1)){
if (n_distinct(df1[[col]]) > 2 & n_distinct(df1[[col]]) < 21){
df1[[col]] <- factor(df1[[col]])
}
}
df1<- dummy.data.frame(df1, dummy.classes = "factor")
for (col in colnames(df1)){
if (class(df1[[col]]) == "factor"){
df1[[col]] <- as.numeric(df1[[col]])
}
}
#------------------Train/Test split----------------------------
set.seed(33)
validation_index<-createDataPartition(df1$Revenue, p=0.7, list=FALSE)
testSet<-df1[-validation_index,]
trainSet<-df1[validation_index,]
load(paste(getwd(),"/ML_models/fit.xgb.RData", sep = ""))
eval <- predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb0 <- cbind(testSet$Revenue, eval) %>%
data.frame() %>%
table() %>% confusionMatrix()
trainSet$Revenue<- as.factor(trainSet$Revenue)
smoted_data <- SMOTE(Revenue ~ ., trainSet, perc.over=100)
smoted_data$Revenue<- as.numeric(smoted_data$Revenue)
load(paste(getwd(),"/ML_models/smote_fit.xgb.RData", sep = ""))
eval <- predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5
confusion_matrix_xgb1 <- cbind(testSet$Revenue,eval) %>%
data.frame() %>%
table() %>% confusionMatrix()
models <- c("XGB", "XGB Smote")
error_class_1 <- round(c(confusion_matrix_xgb0$table[2,1] /
(confusion_matrix_xgb0$table[2,1] + confusion_matrix_xgb0$table[2,2]),
confusion_matrix_xgb1$table[2,1] /
(confusion_matrix_xgb1$table[2,1] + confusion_matrix_xgb1$table[2,2])),4)
logloss_ <- round(c(logLoss(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
logLoss(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
auc_ <- round(c(auc(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
auc(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)
precision_ <- round(c(precision(testSet$Revenue,predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 ),
precision(testSet$Revenue,predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5)),4)
recall_ <- round(c(recall(testSet$Revenue,predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 ),
recall(testSet$Revenue,predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5 )),4)
eval_models_xgb <- data.frame(models, auc_, logloss_, error_class_1, precision_, recall_, stringsAsFactors = F) %>%
arrange(desc(auc_))
eval_models_xgb # XGBoost Smote Wins!!
#-----------Compare Confusion Matrix------------------------
mplot_conf(tag = testSet$Revenue,
score =predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]],model_name = "XGBoost Smote")
confusion_matrix_xgb1
recall_
