---
title: "Online Shoppers Intention"
subtitle: "Business Report"
author: "Pedro López-Escribano Zamora"
date: "1/12/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = FALSE, warning=FALSE, message=FALSE}
#LIbrarys
library(ellipsis)
library(tidyverse)
library(broom)
library(purrr)
library(dummies)
library(scales)
library(GGally)
library(corrplot)
library(gridExtra)
library(grid)
library(h2o)
library(xgboost)
library(ggpubr)
library(caret)
library(Metrics)
library(e1071)
library(Ckmeans.1d.dp)
library(usethis)
library(glue)
library(lares)
library(modelplotr)
library(devtools)
```

```{r, echo=FALSE, warning=FALSE}
data <- read.csv("raw_data/online_shoppers_intention.csv")
nuniques <- as.data.frame(sapply(data, function(x) length(unique(x))))
colnames(nuniques) <- "value"

#Fiels between 3 and 17 unique values will be transform to factor
for (row in row.names(nuniques)){
    if (nuniques[row,] < 18 & nuniques[row,] > 2)
        data[[row]] <- factor(data[[row]])
}

#--------------------------------Missing data------------------------------
#14 rows with NA will be remove
df <- drop_na(data)

#Transform Month column to ordered factor to make nice plots
df$Month<- fct_relevel(df$Month, c("Feb", "Mar", "May", "June", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))
df$Month<- factor(df$Month, ordered = T)
#-------------------------------------------


```

# Proyect Description

## General objective 
* Analyze the behavior and purchases of users in an online store and determine which characteristics have more weight to make a purchase or not.

## Specific objective 
* Create a model that allows estimating the probability a user has to make a purchase, in this way commercial decisions can be made, such as offering specific offers or recommending a series of products similar to those that the user is looking for.

## Data Description

* The dataset consists of 10 numerical and 8 categorical attributes, with `r nrow(df)` observation (users).
* The **'Revenue'** attribute is a Boolean field, where TRUE means that the user bought an item. This attribute is what we want to predict with Machine Learning models.
* **"Administrative", "Administrative Duration", "Informational", "Informational Duration", "Product Related" and "Product Related Duration"** represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another. 
* The **"Bounce Rate"**, **"Exit Rate"** and **"Page Value"** features represent the metrics measured by "Google Analytics" for each page in the e-commerce site. 
+ The value of **"Bounce Rate"** feature for a web page refers to the percentage of visitors who enter the site from that page and then leave ("bounce") without triggering any other requests to the analytics server during that session. 
+ The value of **"Exit Rate"** feature for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session.
+ The **"Page Value"** feature represents the average value for a web page that a user visited before completing an e-commerce transaction.
* The **"Special Day"** feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8.
* The dataset also includes **operating system, browser, region, traffic type, visitor type** as returning or new visitor, a Boolean value indicating whether the date of the visit is **weekend**, and **month** of the year, we have ten months, January and April are missing, we don't know if February and December are complete.

# Exploratory Data Analysis

## Patterns discovered by EDA

### Month  

```{r, echo=FALSE, fig.width=14, fig.height=14}
plot_var_Revenue<- function(x){
  group<- df %>% group_by(df[,x], Revenue) %>% summarize(n = n()) %>% 
    mutate(prop = n/sum(n)) %>% rename(var = "df[, x]")
  
  g0<-ggplot(group,aes(x = var, y = prop, fill = Revenue)) + 
    geom_col() +
    geom_text(aes(label = percent(prop)), vjust = -0.5) + 
    scale_y_continuous(labels = scales::percent) + 
    labs(x = x,
         title = paste("Revenue percent by",x)) + 
    theme(legend.position="bottom")
  
  prop_count<- df %>% group_by(df[,x]) %>% summarize(n = n()) %>% 
    mutate(prop = n/sum(n)) %>% rename(var = "df[, x]")
  
  g1<-ggplot(prop_count, aes(x = var, y=prop)) + 
    geom_col() + 
    geom_text(aes(label = percent(prop)), vjust = -0.5) +
    scale_y_continuous(labels = scales::percent) +
    labs(x = x,
         title = paste("Visitors proportion by", x) ) 
  
  prop_count_revenue_T<- df %>%filter(Revenue == T) %>%group_by(Month) %>% summarize(n = n()) %>% 
    mutate(prop = n/sum(n))
  
  g2<- ggplot(prop_count_revenue_T, aes(x = Month, y=prop)) + 
    geom_col(fill = "#00BA38") + 
    geom_text(aes(label = percent(prop)), vjust = -0.5) +
    scale_y_continuous(labels = scales::percent) +
    labs(x = x,
         title = paste("Proportion of purchases by Month"))

  grid.arrange(g1, g2, g0, nrow = 3, ncol =1)
}

plot_var_Revenue("Month")

```


```{r, echo=FALSE}
prop_count_revenue_T<- df %>%filter(Revenue == T) %>%group_by(Month) %>% summarize(n = n()) %>% 
  mutate(prop = n/sum(n))


prop_count<- df %>% group_by(Month) %>% summarize(n = n()) %>% 
  mutate(prop = n/sum(n))

count_sales_corr<- cor.test(prop_count_revenue_T$prop, prop_count$prop)

```

* We can observe a relationship between the number of visitors and the number of sales, the result of the correlation test, correlation on average is **`r round(count_sales_corr$estimate[[1]],2)`** with a 95% confidence interval **[`r round(count_sales_corr$conf.int[1],2)` , `r round(count_sales_corr$conf.int[2],2)`]**

* We only have 10 pairs, one for each month, we can create a simple regression model to estimate the benefit proportion based on the proportion of visitors, the most optimal way to do this task would be to create a model for each month, but since We only have 10 months of 1 year this would not make sense. Anyway we can create the model and make predictions only for the months with a prediction error (residual) close to 0.

```{r, echo=FALSE}
df_prop<-data.frame(prop_count_revenue_T$prop,prop_count$prop)
names(df_prop) <- c("prop_revenue_T", "prop_count")
df_prop<- cbind(levels(df$Month),df_prop) %>% rename(Month = "levels(df$Month)")

mode_lm<-lm(prop_revenue_T~prop_count, data = df_prop)
df_lm<- augment(mode_lm, df_prop)
df_lm %>% select(Month:.fitted, .resid) 

```

* June and July are the months in which we can expect our prediction to be most accurate. Now we can answer the question: What proportion of sales can we obtain if we manage to increase the proportion of visits by 5% in June or July?

```{r,echo=FALSE}
pred_months<- df_lm %>% select(Month:prop_count) %>% filter(Month %in% c("June", "Jul"))
prop_count_fixed <- c( pred_months$prop_count + 0.05)
pred_prop_revenue<-predict(mode_lm, newdata = list(prop_count = pred_months$prop_count + 0.05))
improvement<- pred_prop_revenue - pred_months$prop_revenue_T
results<- tibble(cbind(pred_months, prop_count_fixed, pred_prop_revenue,improvement))
results
```

* If we manage to increase June or July visits by **5%**, we can expect to see an improvement of  **`r round(results$improvement[1]*100,2)`%** and **`r round(results$improvement[2]*100,2)`%** respectively


### Visitor Type

```{r echo=FALSE, fig.width=14, fig.height=10}
plot_var_Revenue<- function(x){
    group<- df %>% group_by(df[,x], Revenue) %>% summarize(n = n()) %>% 
        mutate(prop = n/sum(n)) %>% rename(var = "df[, x]")
    
    g0<-ggplot(group,aes(x = var, y = prop, fill = Revenue)) + 
        geom_col() +
        geom_text(aes(label = percent(prop)), vjust = -0.5) + 
        scale_y_continuous(labels = scales::percent) + 
        labs(x = x,
             title = paste("Revenue percent by",x)) + 
        theme(legend.position="bottom")
    
    prop_count<- df %>% group_by(df[,x]) %>% summarize(n = n()) %>% 
        mutate(prop = n/sum(n)) %>% rename(var = "df[, x]")
    
    g1<-ggplot(prop_count, aes(x = var, y=prop)) + 
        geom_col() + 
        geom_text(aes(label = percent(prop)), vjust = -0.5) +
            scale_y_continuous(labels = scales::percent) +
                labs(x = x,
                 title = paste("Visitors proportion by", x) ) +
        expand_limits(y = seq(0,1, by = .1))
        
    
    grid.arrange(g1, g0, nrow = 2, ncol =1)
}
plot_var_Revenue("VisitorType")
```

* New visitors represent **14%**, of this percentage **25%** buy an item, while returning visitors represent **86%**, of this percentage **14%** buy an item.
* The value "Other" represents values that we do not know if they belong to one field or another.

## Numerical variables distribution

* We have 10 numerical variables, in 3 of them we can see clear differences between the distributions if we filter each distribution for Revenue = True and Revenue = False.

* I am going to use 3 different plots to analyze the distributions, each with its advantages.
  + **ECDF** Essentially allows you to plot a feature of your data in order from smallest to largest and view the entire feature as if it were distributed across the data set. Personally I prefer to use this type of plots instead of using histograms, histograms can lead to biases, depending on how the bins are configured the interpretation may be different.
  + **Boxplot** are useful for very sparse distributions, box determines the interquartile range, the line in the box indicates the median and the points indicate outliers
  + **Jitterplot** are a variation of the scatterplot. It adds a small amount of random variation to the location of each point and is a useful way to handle overlap.

### Exit Rate

```{r, echo=FALSE, warning=FALSE, fig.width=14, fig.height=8}
  num_var_dist <- function(y){
    jit<- ggplot(df,aes(x = Revenue, y = df[[y]])) + 
        geom_jitter(aes(color = Revenue),show.legend = F) + 
        labs(y = y,
             title = "Jitterplot") +
        theme(axis.title.y = element_blank())
    
    box<- ggplot(df,aes(x = Revenue, y = df[[y]])) + 
        geom_boxplot(aes(fill = Revenue),show.legend = F) + 
        labs(y = y,
             title = "Boxplot")
    ecdf<- ggplot(df,aes(df[[y]])) + 
        stat_ecdf(aes(color = Revenue)) +
        labs(x = y,
             y = "Probs",
             title = "ECDF") + 
        expand_limits(x = seq(0, max(df[[y]]), by = 1))
    
    grid.arrange(ecdf,arrangeGrob( box, jit ,ncol = 2), nrow = 2)
    
  }

num_var_dist("ExitRates")

```

* We can see a couple of interesting facts
  + **25%** of the values for Revenue = False are above 0.05 and for Revenue = True only **5%** of the values are above 0.05.
  + As we can expect the median in Revenue = False is higher.
  + We can observe in the jitterplot = True 3 outliers at 0.2.
  
### Bounce Rate

```{r, echo=FALSE, warning=FALSE, fig.width=14, fig.height=8}
  num_var_dist("BounceRates")
  cor_BR_ER<-cor.test(df$BounceRates, df$ExitRates)

```

* The differences that we can find here are very similar to the previous plot, it is not a surprise knowing that these variables are highly correlated, with a correlation coefficient of **`r round(cor_BR_ER$estimate[[1]],3)`**

### Page Value

```{r, echo=FALSE, warning=FALSE, fig.width=14, fig.height=8}
  num_var_dist("PageValues")

```

* Without a doubt "PageValues" is the most relevant attribute to observe differences between Revenue values.
  + For Revenue = True, only **22%** of the values are 0 and we have **18** observations above 200, while for Revenue = False **88%** of the values are 0 and we only have **2** values above 200.
  
# Machine Learning

## Introduction

* I am going to apply machine learning classification algorithms that are part of supervised learning, in other words, a field is required which has labels to be able to train the algorithm, our field will be Revenue, since we want to predict when a customer makes a purchase.

* As we have already seen, Revenue can be False or True, we are interested in predicting as accurately as possible when this value will be True.

* I have used the H2o API to run different models, the Caret package to implement and optimize the XGBoost algorithm and the Lares library that allows using the H2O API and facilitates the use of some graphics to understand the commercial value of the algorithm.

## Techniques used

* I have used 3 techniques to avoid overtraining
  + Train/Test split.
  + Stratified cross validation.
  + Early stop (for boosting based algorithms).
* The algorithms are evaluated with 3 metrics
  + **AUC** (area under the curve)  If we obtain a perfect classifier, then the AUC score is 1.0. If the classifier is random in its guesses, then the AUC score is 0.5.
  + **LogLoss**  it is a measure of uncertainty (you may call it entropy), so a low Log Loss means a low uncertainty/entropy of your model. Log Loss is similar to the Accuracy, but it will favor models that distinguish more strongly the classes.
  + **Error class 1** it is the proportion of error of the algorithm with respect to class 1 (Revenue = True).
  
### H2o connection

```{r, echo=FALSE}
h2o.init(
    ip = "localhost",
    nthreads = -1,
    max_mem_size = "4g"
)
```
  
## Model Evaluation

```{r, echo=FALSE, message=FALSE, warning=FALSE}

#--------------------------XGBoost with Caret-----------------------------
#-------------Data preparation---------------

df$Month <- as.numeric(df$Month)
df1<-df
#-------Transform all categorical variables to factor, to allows us convert it to dummies variables
for (col in colnames(df1)){
    if (n_distinct(df1[[col]]) > 2 & n_distinct(df1[[col]]) < 21){
        df1[[col]] <- factor(df1[[col]])
    }
}

df1<- dummy.data.frame(df1, dummy.classes = "factor")
for (col in colnames(df1)){
    if (class(df1[[col]]) == "factor"){
        df1[[col]] <- as.numeric(df1[[col]])
    }
}
#------------------Train/Test split----------------------------
set.seed(33)
validation_index<-createDataPartition(df1$Revenue, p=0.8, list=FALSE)
testSet<-df1[-validation_index,]
trainSet<-df1[validation_index,]

#-----------------------------LOAD MODELS----------------------------------------------------

model1 <- h2o.loadModel(path = "ML_models/StackedEnsemble_BestOfFamily_AutoML_20201021_084846")
model2 <- h2o.loadModel(path = "ML_models/GBM_grid__1_AutoML_20201021_084846_model_60")
model3 <- h2o.loadModel(path = "ML_models/GBM_grid__1_AutoML_20201021_084846_model_32")

model_dp <- h2o.loadModel(path = "ML_models/model_dp")
model_glm <- h2o.loadModel(path = "ML_models/model_glm")

load(paste(getwd(),"/ML_models/fit.xgb.RData", sep = ""))
#--------------------------EVALUATION--------------------------------

eval <- predict(fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5

confusion_matrix_xgb0 <- cbind(testSet$Revenue, eval) %>% 
    data.frame() %>%
    table() %>% confusionMatrix()

models <- c("Stacked Ensemble auto", "GBM0 auto","GBM1 auto","GLM","Deep Learning","XGB")

error_class_1 <- round(c(h2o.confusionMatrix(model1, valid=T)[,3][2],
                  h2o.confusionMatrix(model2, valid=T)[,3][2],
                  h2o.confusionMatrix(model3, valid=T)[,3][2],
                  h2o.confusionMatrix(model_glm, valid=T)[,3][2],
                  h2o.confusionMatrix(model_dp, valid=T)[,3][2],
                  confusion_matrix_xgb0$table[2,1] / 
                      (confusion_matrix_xgb0$table[2,1] + confusion_matrix_xgb0$table[2,2])),4)

logloss_ <- round(c(h2o.logloss(h2o.performance(model1, valid = T)),
              h2o.logloss(h2o.performance(model2, valid = T)),
              h2o.logloss(h2o.performance(model3, valid = T)),
              h2o.logloss(h2o.performance(model_glm, valid = T)),
              h2o.logloss(h2o.performance(model_dp, valid = T)),
              logLoss(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]])),4)
              
auc_ <- round(c(h2o.auc(h2o.performance(model1, valid = T)),
          h2o.auc(h2o.performance(model2, valid = T)),
          h2o.auc(h2o.performance(model3, valid = T)),
          h2o.auc(h2o.performance(model_glm, valid = T)),
          h2o.auc(h2o.performance(model_dp, valid = T)),
          auc(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]])),4)


eval_models <- data.frame(models, auc_, logloss_, error_class_1, stringsAsFactors = F) %>%
    arrange(desc(auc_))
eval_models
```  

* The table is sorted in descending order by auc_, models with "auto" termination are created automatically with h2o. The winning model is XGBoost, it should be noted that with a manual hyperparameter configuration it was already the best, but also made a search for the best hyperparameters using Grid Search.

* Next I test h2o with the Lares library that allows us to configure some more parameters, I will select the best model and compare it with XGBoost

```{r, include=FALSE}
lares_auto<-h2o_automl(df, y = "Revenue", target = "TRUE" , split = 0.8, balance = TRUE, scale = TRUE, max_models = 10,
                       exclude_algos = NULL, project ="Online shoppers intentions", seed = 33,
                       stopping_metric = "AUC", stopping_rounds = 5, no_outliers = T)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#---------------------------------------Evaluation best models-------------------------------------
best_models <- c("GBM","XGBoost")

error_class_1 <- round(c(h2o.confusionMatrix(lares_auto$model,)[,3][2],
                   confusion_matrix_xgb0$table[2,1] / 
                       (confusion_matrix_xgb0$table[2,1] + confusion_matrix_xgb0$table[2,2])),4)

logloss_ <- round(c(h2o.logloss(h2o.performance(lares_auto$model, xval = T)),
            logLoss(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]])),4)

auc_ <- round(c(h2o.auc(h2o.performance(lares_auto$model, xval = T)),
        auc(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]])),4)

eval_best_models <- data.frame(best_models, auc_, logloss_, error_class_1) %>% arrange(desc(auc_))
eval_best_models

```

* We have the same AUC, but logloss and error class 1 are lower in XGBoost, so we will use XGBoost

* Although we have unbalanced classes (85% for Revenue = FALSE and 15% for Reveneu = TRUE), the results are not bad, we will try to improve them using the SMOTE method to balance the classes 

```{r, echo= FALSE}
load(paste(getwd(),"/ML_models/smote_fit.xgb.RData", sep = ""))

eval <- predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]] > 0.5

confusion_matrix_xgb1 <- cbind(testSet$Revenue,eval) %>% 
    data.frame() %>%
    table() %>% confusionMatrix()

models <- c("XGB", "XGB Smote")

error_class_1 <- round(c(confusion_matrix_xgb0$table[2,1] / 
                             (confusion_matrix_xgb0$table[2,1] + confusion_matrix_xgb0$table[2,2]),
                         confusion_matrix_xgb1$table[2,1] / 
                             (confusion_matrix_xgb1$table[2,1] + confusion_matrix_xgb1$table[2,2])),4)

logloss_ <- round(c(logLoss(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
                    logLoss(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)

auc_ <- round(c(auc(testSet$Revenue, predict(fit.xgb, testSet,type = "prob")[["second_class"]]),
                auc(testSet$Revenue, predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])),4)


eval_models_xgb <- data.frame(models, auc_, logloss_, error_class_1, stringsAsFactors = F) %>%
    arrange(desc(auc_))

eval_models_xgb 

```

* AUC is basically the same, logloss does not improve, but we have managed to reduce the classification error in our target class by more than half, so we will use XGBoost with Smote


* Let's see which variables are the most important in our model

```{r, echo=FALSE, warning=FALSE, message=FALSE}
xgb_varImp <- varImp(smote_fit.xgb,scale = F)$importance
xgb_varImp <- rownames_to_column(xgb_varImp,"Variables")
ggplot(head(xgb_varImp,10), aes(x= Overall, y=fct_reorder(Variables, Overall))) + 
    geom_col() + 
        labs(x = "Importance",
             y = "Variable",
             title = "XGBoost Smote Variables Importance")

```

* As we expected after observing the distributions, by far the most important variable is **Page Value**.

## Business evaluation

### Let's answer some business questions.
* When we apply the model and select the best X deciles, what % of the actual target class observations can we expect to target?
  
```{r, echo=FALSE, warning=FALSE, message=FALSE}
mplot_gain(tag = testSet$Revenue, 
           score =predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])

```

  + When we select the **10%** with the highest probability according to XGBoost, this selection contains **53%** of all users who make a purchase in test data.
  
* When we apply the model and select up until decile X, what is the expected % of target class observations in the selection?

```{r, echo=FALSE, warning=FALSE, message=FALSE}
mplot_response(tag = testSet$Revenue, 
           score =predict(smote_fit.xgb, testSet,type = "prob")[["second_class"]])

```

  + When we select deciles 1 until 3 according to model XGBoost in test data the % of users who make a purchase in the selection is **49%**.
  

**Thanks for your interest!!**
